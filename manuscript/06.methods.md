## Methods

All analysis was implemented in Python v3.8 and can be found at <https://github.com/meyer-lab/systemsSerology>.

### Total Matrix-Tensor Factorization

We decomposed the systems serology measurements into a reduced series of Kruskal-formatted factors. Tensor operations were defined using Tensorly [@arXiv:1610.09555]. To capture the structure of the data, where the majority of measurements were made for specific antigens, but antibody glycosylation was measured in an antigen-generic form, we separated these two types of data into separate tensors, with shared patient-dimension factors:

$$X_{antigen} \approx \sum_{r=1}^R a_r \circ b_r \circ c_r$$

$$X_{glycosylation} \approx \sum_{r=1}^R a_r \circ d_r$$
where $a_r$, $b_r$, and $c_r$ are vectors indicating variation along the patient, receptor, and antigen dimensions, respectively. $d_r$ is a vector indicating variation along glycan forms within the glycan matrix.

Decomposition was performed through an alternating least squares (ALS) scheme [@doi:10.1137/07070111X]. Each least squares step was performed separately for each slice along a given mode, with missing values removed. While this made each iteration step much slower, convergence was much faster as a consequence of requiring fewer iterations. Missing values did not strictly follow a tensor slice pattern, and so alternative approaches such as a sampling Khatri-Rao product were disregarded as they would still require iterative filling [@doi:10.1137/17M1112303]. Filling in missing values on each iteration required many more iterations due to a high fraction of missing values (43%). The ALS iterations were repeated until the improvement in R2X over the last ten iterations was less than $1\times 10^{-7}$.

In order to enforce shared factors along the patient dimension, the antigen tensor and glycan matrix were concatenated after tensor unfolding. The Khatri-Rao product of the receptor and antigen factors was similarly concatenated to the glycan factors. The least-squares solution on this axis therefore solved for minimizing the squared error across both data compendiums. The other dimensions were solved using a standard ALS approach.

Missing values within the antigen-specific tensor were arranged in a specific pattern with respect to the patient dimension. That is, if a particular receptor-antigen pair measurement was made for any one patient, it was made for every patient in the cohort. This structure allowed us to remove many missing values by removing these tensor chords from both the Katri-Rao product and unfolded tensor when solving for the patient mode matrix. This slightly accelerated solving along this dimension.

### Reconstruction Fidelity

To calculate the fidelity of our factorization results, we calculated the percent variance explained. First, the total variance was calculated by summing the variance in both the antigen-specific tensor and glycan matrix:
$$v_{total} = \left \| X_{antigen}  \right \| + \left \| X_{glycosylation}  \right \|$$
Any missing values were ignored in the variance calculation throughout. Then, the remaining variance after taking the difference between the original data and its reconstruction was calculated:
$$v_{r,antigen} = \left \| X_{antigen} - \hat X_{antigen}  \right \|$$
An analogous equation was used for the glycan matrix. Finally, the fraction of variance explained was calculated:
$$R2X = 1 - \frac{v_{r,antigen} + v_{r,glycosylation}}{v_{total}}$$
Where indicated, this quantity was calculated for values left out to assess the fidelity of imputation. In these cases this quantity was only calculated on those left out values, and was indicated as Q2X.

### Cross-Validation

Unlike in earlier work [@DOI:10.15252/msb.20177881], we employed a nested, 20-fold cross-validation strategy, wherein the regularization extent was determined separately within each cross-validation fold. Hyperparameter selection outside of each cross-validation fold is known to consistently lead to model over-fitting [@overfit]. When evaluating predictions using the factorization, both regularized linear and Gaussian process models were built, and the better performing model selected.

### Logistic Regression / Elastic Net

Logistic regression and elastic net were performed using `LogisticRegressionCV` and `ElasticNetCV` implemented within `scikit-learn` [@scikit]. Both methods used 10-fold cross-validation to select the regularization strength with minimal cross-validation error, and a fraction of l1 to l2 regularization equal to 0.8 to match previous results [@DOI:10.15252/msb.20177881]. Logistic regression used the SAGA solver [@arXiv:1407.0202]. Elastic net regression was set to normalize the data before model assembly.

### Gaussian Process Regression / Classification

Gaussian process regression or classification was performed where indicated using the implementation within `scikit-learn` [@scikit]. Classification was performed with warm restarting. Regression was performed with output scaling. In both cases, an radial basis function kernel was used with a constant scaling factor and additive white noise. The kernel's parameters were left unbounded.

### Principal Components Analysis

Principal components analysis was performed using the implementation within the Python package `statsmodels` and the SVD-based solver. Missing values were handled by an expectation-maximization approach, wherein they were filled in with the imputed value. This filling step was performed up to 100 iterations until convergence as determined by a tolerance of $1 \times 10^{-5}$.

### Missingness Imputation

To evaluate the ability of factorization to impute missing data, we introduced new missing values by removing chords from the antigen-specific tensor, and then looking at the variance explained on reconstruction (Q2X). More specifically, ten randomly selected receptor-antigen pairs were removed and marked as missing across all patients. TMTF decomposition was performed as described above, and then these left out data were compared to the reconstructed values. This process was repeated for the same chords across varying numbers of components.

TODO: Provide description of source data and methods.
